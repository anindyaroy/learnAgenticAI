{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anindyaroy/learnAgenticAI/blob/main/M3_LLM_IO_LLMs_and_Chat_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVraWpDTW8i0"
      },
      "source": [
        "# Exploring LLMs and ChatModels for LLM Input / Output with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install OpenAI, HuggingFace and LangChain dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgGBD8bqiV4s"
      },
      "outputs": [],
      "source": [
        "# Updated package installations for better performance and compatibility:\n",
        "!pip install langchain==0.3.26\n",
        "!pip install langchain-openai==0.3.28\n",
        "!pip install langchain-community==0.3.27\n",
        "!pip install huggingface_hub==0.33.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikRH0Xh-BS3q"
      },
      "outputs": [],
      "source": [
        "# Don't run if you want to use only chatgpt\n",
        "# This is for accessing open LLMs from huggingface\n",
        "!pip install transformers==4.53.3\n",
        "!pip install langchain_huggingface==0.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtBa7rlWJWH3"
      },
      "source": [
        "## Enter API Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6RD7As2sm8G"
      },
      "source": [
        "#### Enter your Open AI Key here\n",
        "\n",
        "You can get the key from [here](https://platform.openai.com/api-keys) after creating an account or signing in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfe6zi6jiV4v"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKlax-updNW-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP7-0nA0s83b"
      },
      "source": [
        "#### Enter your HuggingFace token here\n",
        "\n",
        "You can get the key from [here](https://huggingface.co/settings/tokens) after creating an account or signing in. This is free."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av1UpSgXZUsI"
      },
      "outputs": [],
      "source": [
        "# skip if only using chatgpt\n",
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass('Please enter your HuggingFace Token here: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5rOqCyianbP"
      },
      "source": [
        "## Setup necessary system environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PIStD04Zp9p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqX0BkkWZ_e0"
      },
      "source": [
        "# Model I/O\n",
        "\n",
        "In LangChain, the central part of any application is the language model. This module provides crucial tools for working effectively with any language model, ensuring it integrates smoothly and communicates well.\n",
        "\n",
        "### Key Components of Model I/O\n",
        "\n",
        "**LLMs and Chat Models (used interchangeably):**\n",
        "- **LLMs:**\n",
        "  - **Definition:** Pure text completion models.\n",
        "  - **Input/Output:** Receives a text string and returns a text string.\n",
        "- **Chat Models:**\n",
        "  - **Definition:** Based on a language model but with different input and output types.\n",
        "  - **Input/Output:** Takes a list of chat messages as input and produces a chat message as output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4h0xywyJ3v7"
      },
      "source": [
        "## Chat Models and LLMs\n",
        "\n",
        "Large Language Models (LLMs) are a core component of LangChain. LangChain does not implement or build its own LLMs. It provides a standard API for interacting with almost every LLM out there.\n",
        "\n",
        "There are lots of LLM providers (OpenAI, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSdF6_R7J45Z"
      },
      "source": [
        "## Accessing Commercial LLMs like ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kai2VrJ1m7q9"
      },
      "source": [
        "\n",
        "\n",
        "### Accessing ChatGPT as an LLM\n",
        "\n",
        "Here we will show how to access a basic ChatGPT Instruct LLM. However the ChatModel interface which we will see later, is better because the LLM API doesn't support the chat models like `gpt-3.5-turbo`and only support the `instruct`models which can respond to instructions but can't have a conversation with you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EtxFCUItXfH"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "chatgpt = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXnZYGu9tkDY"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Explain what is Generative AI in 3 bullet points\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS5rdBbntqMf"
      },
      "outputs": [],
      "source": [
        "response = chatgpt.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC3q5PBevVVs"
      },
      "source": [
        "### Accessing ChatGPT as an Chat Model LLM\n",
        "\n",
        "Here we will show how to access the more advanced ChatGPT Turbo Chat-based LLM. The ChatModel interface is better because this supports the chat models like `gpt-3.5-turbo`which can respond to instructions as well as have a conversation with you. We will look at the conversation aspect slightly later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8nnrOGxZ2uZ"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDJ8dDFlbWP4"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Explain what is Generative AI in 3 bullet points\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYym2ioWb-Mw"
      },
      "outputs": [],
      "source": [
        "response = chatgpt.invoke(prompt)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uehA4Tcdco8G"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QfmhognHWnR"
      },
      "source": [
        "## Accessing Open Source LLMs with HuggingFace and LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZgRpioKAKn"
      },
      "source": [
        "### Accessing Open LLMs with HuggingFace Serverless API\n",
        "\n",
        "The free [serverless API](https://huggingface.co/inference-api/serverless) lets you implement solutions and iterate in no time, but it may be rate limited for heavy use cases, since the loads are shared with other requests.\n",
        "\n",
        "For enterprise workloads, you can use Inference Endpoints - Dedicated which would be hosted on a specific cloud instance of your choice and would have a cost associated with it. Here we will use the free serverless API which works quite well in most cases.\n",
        "\n",
        "The advantage is you do not need to download the models or run them locally on a GPU compute infrastructure which takes time and also would cost you a fair amount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6X-30aHK7rU"
      },
      "source": [
        "#### Accessing Microsoft Phi-3 Mini Instruct\n",
        "\n",
        "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. Check more details [here](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(HUGGINGFACEHUB_API_TOKEN)"
      ],
      "metadata": {
        "id": "uL0I9d_IxyEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fgv8DhwcCWD"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "repo_id = \"microsoft/Phi-3-mini-4k-instruct\"  #\"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "phi3_params = {\n",
        "                  # \"do_sample\": False, # greedy decoding - temperature = 0\n",
        "                  # \"return_full_text\": False, # don't return input prompt\n",
        "                  \"max_new_tokens\": 1000, # max tokens answer can go upto\n",
        "                }\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    # max_length=128,\n",
        "    temperature=0.5,\n",
        "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
        "   **phi3_params\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LMB-dezWeRR"
      },
      "outputs": [],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxVTo6BxC420"
      },
      "outputs": [],
      "source": [
        "# Phi3 expects input prompt to be formatted in a specific way\n",
        "# check more details here: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
        "phi3_prompt = \"\"\"<|user|>Explain what is Generative AI in 3 bullet points<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "print(phi3_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import model_info\n",
        "\n",
        "info = model_info(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "print(info.pipeline_tag)\n",
        "\n"
      ],
      "metadata": {
        "id": "g8hsa2-a0Im9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = llm.invoke(phi3_prompt)\n",
        "# print(response)\n",
        "\n",
        "# response = llm(phi3_prompt)\n",
        "# print(response)\n",
        "\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
        "    max_new_tokens=1000,\n",
        "    temperature=0.5,\n",
        ")\n",
        "\n",
        "phi3_prompt = \"<|user|>Explain what is Generative AI in 3 bullet points<|end|>\\n<|assistant|>\"\n",
        "\n",
        "response = llm(phi3_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "3kx6cPugOD1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke({\"question\": \"Explain what is Generative AI in 3 bullet points\"})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "v5u73VPwNjN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy2DDI6He97g"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(phi3_prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpNplonmLTXT"
      },
      "source": [
        "#### Accessing Google Gemma 2B Instruct\n",
        "\n",
        "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure. Check more details [here](https://huggingface.co/google/gemma-1.1-2b-it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SHuEt91DbtX"
      },
      "outputs": [],
      "source": [
        "gemma_repo_id = \"google/gemma-2b-it\"\n",
        "\n",
        "gemma_params = {\n",
        "                  \"do_sample\": False, # greedy decoding - temperature = 0\n",
        "                  \"return_full_text\": False, # don't return input prompt\n",
        "                  \"max_new_tokens\": 1000, # max tokens answer can go upto\n",
        "                }\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=gemma_repo_id,\n",
        "    **gemma_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vslmW1HBE7mA"
      },
      "outputs": [],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bhVrcuGEFHqv",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UPxTNk6LxFn"
      },
      "source": [
        "### Accessing Local LLMs with HuggingFacePipeline API\n",
        "\n",
        "Hugging Face models can be run locally through the `HuggingFacePipeline` class. However remember you need a good GPU to get fast inference\n",
        "\n",
        "The Hugging Face Model Hub hosts over 500k models, 90K+ open LLMs\n",
        "\n",
        "These can be called from LangChain either through this local pipeline wrapper or by calling their hosted inference endpoints through the `HuggingFaceEndpoint` API we saw earlier.\n",
        "\n",
        "To use, you should have the `transformers` python package installed, as well as `pytorch`.\n",
        "\n",
        "Advantages include the model being completely local, high privacy and security. Disadvantages are basically the necessity of a good compute infrastructure, preferably with a GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53E-Yf9GWFPH"
      },
      "source": [
        "#### Accessing Google Gemma 2B and running it locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5rp8JNEiV44"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbK8ShKBGFFv"
      },
      "outputs": [],
      "source": [
        "gemma_params = {\n",
        "                  \"do_sample\": False, # greedy decoding - temperature = 0\n",
        "                  \"return_full_text\": False, # don't return input prompt\n",
        "                  \"max_new_tokens\": 1000, # max tokens answer can go upto\n",
        "                }\n",
        "\n",
        "local_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"google/gemma-1.1-2b-it\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=gemma_params,\n",
        "    # device=0 # when running on Colab selects the GPU, you can change this if you run it on your own instance if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQjsZdU7Me0f"
      },
      "outputs": [],
      "source": [
        "local_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSZ-xicBMqL1"
      },
      "outputs": [],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rShVPhwTNc9J"
      },
      "outputs": [],
      "source": [
        "# Gemma2B when used locally expects input prompt to be formatted in a specific way\n",
        "# check more details here: https://huggingface.co/google/gemma-1.1-2b-it#chat-template\n",
        "gemma_prompt = \"\"\"<bos><start_of_turn>user\\n\"\"\" + prompt + \"\"\"\\n<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "print(gemma_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Giw-QVPFMrLt"
      },
      "outputs": [],
      "source": [
        "response = local_llm.invoke(gemma_prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O-6EzTvWQ56"
      },
      "source": [
        "### Accessing Open LLMs in HuggingFace as a Chat Model LLM\n",
        "\n",
        "Here we will show how to access open LLMs from HuggingFace like Google Gemma 2B and make them have a conversation with you. We will look at the conversation aspect slightly later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auoQdEoNd8l2"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "chat_gemma = ChatHuggingFace(llm=llm,\n",
        "                             model_id='google/gemma-1.1-2b-it')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhrRC7HiiKAR"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7kLCJevjCe_"
      },
      "source": [
        "## Message Types for ChatModels and Conversational Prompting\n",
        "\n",
        "Conversational prompting is basically you, the user, having a full conversation with the LLM. The conversation history is typically represented as a list of messages.\n",
        "\n",
        "ChatModels process a list of messages, receiving them as input and responding with a message. Messages are characterized by a few distinct types and properties:\n",
        "\n",
        "- **Role:** Indicates who is speaking in the message. LangChain offers different message classes for various roles.\n",
        "- **Content:** The substance of the message, which can vary:\n",
        "  - A string (commonly handled by most models)\n",
        "  - A list of dictionaries (for multi-modal inputs, where each dictionary details the type and location of the input)\n",
        "\n",
        "Additionally, messages have an `additional_kwargs` property, used for passing extra information specific to the message provider, not typically general. A well-known example is `function_call` from OpenAI.\n",
        "\n",
        "### Specific Message Types\n",
        "\n",
        "- **HumanMessage:** A user-generated message, usually containing only content.\n",
        "- **AIMessage:** A message from the model, potentially including `additional_kwargs`, like `tool_calls` for invoking OpenAI tools.\n",
        "- **SystemMessage:** A message from the system instructing model behavior, typically containing only content. Not all models support this type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DAnyVy1QzJ7"
      },
      "source": [
        "## Conversational Prompting with ChatGPT\n",
        "\n",
        "Here we use the `ChatModel` API in `ChatOpenAI` to have a full conversation with ChatGPT while maintaining a full flow of the historical conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6GBvzSHRDWA"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKDMkowiiNjG"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "prompt = \"\"\"Can you explain what is Generative AI in 3 bullet points?\"\"\"\n",
        "sys_prompt = \"\"\"Act as a helpful assistant and give meaningful examples in your responses.\"\"\"\n",
        "messages = [\n",
        "    SystemMessage(content=sys_prompt),\n",
        "    HumanMessage(content=prompt),\n",
        "]\n",
        "\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xizcAMNKjYSk"
      },
      "outputs": [],
      "source": [
        "response = chatgpt.invoke(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrYfYw8Ijhjz"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5qvveyhjksA"
      },
      "outputs": [],
      "source": [
        "# add the past conversation history into messages\n",
        "messages.append(response)\n",
        "# add the new prompt to the conversation history list\n",
        "prompt = \"\"\"What did we discuss so far?\"\"\"\n",
        "messages.append(HumanMessage(content=prompt))\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8dQjk-Cj15m"
      },
      "outputs": [],
      "source": [
        "# sent the conversation history along with the new prompt to chatgpt\n",
        "response = chatgpt.invoke(messages)\n",
        "response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3ru23E2RZtT"
      },
      "source": [
        "## Conversational Prompting with Open LLMs via HuggingFace\n",
        "\n",
        "Here we use the `ChatModel` API in `ChatHuggingFace` to have a full conversation with any open LLMs while maintaining a full flow of the historical conversations. Here we use the Google Gemma 2B LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oQOecFzpoBm"
      },
      "outputs": [],
      "source": [
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm6IYKhOR38a"
      },
      "outputs": [],
      "source": [
        "# not needed if you are only running chatgpt\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "chat_gemma = ChatHuggingFace(llm=llm,\n",
        "                             model_id='google/gemma-1.1-2b-it')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHsTawgsj4Nd"
      },
      "outputs": [],
      "source": [
        "# this runs prompts using the open LLM - however gemma doesnt support a system prompt\n",
        "prompt = \"\"\"Explain Deep Learning in 3 bullet points\"\"\"\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(content=prompt),\n",
        "]\n",
        "\n",
        "response = chat_gemma.invoke(messages) # doesn't support system prompts\n",
        "messages.append(response)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiQT6kt0iV48"
      },
      "outputs": [],
      "source": [
        "# this runs prompts using the open LLM - however gemma doesnt support a system prompt\n",
        "prompt = \"\"\"Explain Deep Learning in 3 bullet points\"\"\"\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(content=prompt),\n",
        "]\n",
        "\n",
        "response = chat_gemma.invoke(messages) # doesn't support system prompts\n",
        "messages.append(response)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvtiCay9QlvQ"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr98rfd6SFVN"
      },
      "outputs": [],
      "source": [
        "# formatting prompt is automatically done inside the chatmodel\n",
        "# formats in this syntax: https://huggingface.co/google/gemma-1.1-2b-it#chat-template\n",
        "print(chat_gemma._to_chat_prompt([messages[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjE2nX35QYe-"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Now do the same for Machine learning\"\"\"\n",
        "messages.append(HumanMessage(content=prompt))\n",
        "\n",
        "response = chat_gemma.invoke(messages) # doesn't support system prompts\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKtpgnViV49"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvRsETZQiV49"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLlIDhRJiV49"
      },
      "outputs": [],
      "source": [
        "huggingface_hub.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEwMhAmqiV49"
      },
      "outputs": [],
      "source": [
        "client = InferenceClient(model='google/gemma-1.1-2b-it')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xclUesARiV49"
      },
      "outputs": [],
      "source": [
        "for message in client.chat_completion(messages=[{'role': 'user', 'content': 'waht is the capital of france?'}],\n",
        "                                      max_tokens=200, stream=True):\n",
        "    print(message.choices[0].delta.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FxPWAA9Tt-JT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}