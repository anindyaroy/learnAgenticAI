{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anindyaroy/learnAgenticAI/blob/main/M3_L10_LangChain_LLM_Input_Output_Output_Parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVraWpDTW8i0"
      },
      "source": [
        "# Output Parsers for LLM Input / Output with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install OpenAI, HuggingFace and LangChain dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG45p4vhBS0f"
      },
      "outputs": [],
      "source": [
        "# Updated package versions as in previous notebooks\n",
        "# Ensures consistent dependencies across the module\n",
        "# !pip install langchain==0.3.21\n",
        "# !pip install langchain-openai==0.3.9\n",
        "# !pip install langchain-community==0.3.19\n",
        "\n",
        "\n",
        "!pip install langchain==0.3.26\n",
        "!pip install langchain-openai==0.3.28\n",
        "!pip install langchain-community==0.3.27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtBa7rlWJWH3"
      },
      "source": [
        "## Enter API Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6RD7As2sm8G"
      },
      "source": [
        "#### Enter your Open AI Key here\n",
        "\n",
        "You can get the key from [here](https://platform.openai.com/api-keys) after creating an account or signing in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ogxBkS6ZnnC"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Please enter your Open AI API Key here: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5rOqCyianbP"
      },
      "source": [
        "## Setup necessary system environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PIStD04Zp9p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4h0xywyJ3v7"
      },
      "source": [
        "## Chat Models and LLMs\n",
        "\n",
        "Large Language Models (LLMs) are a core component of LangChain. LangChain does not implement or build its own LLMs. It provides a standard API for interacting with almost every LLM out there.\n",
        "\n",
        "There are lots of LLM providers (OpenAI, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSdF6_R7J45Z"
      },
      "source": [
        "## Accessing Commercial LLMs like ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8nnrOGxZ2uZ"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cei3lVfHduK"
      },
      "source": [
        "## Output Parsers\n",
        "Output parsers are essential in Langchain for structuring the responses from language models. Below, we will discuss the role of output parsers and include examples using Langchain's specific parser types: PydanticOutputParser, JsonOutputParser, and CommaSeparatedListOutputParser.\n",
        "\n",
        "- **Pydantic parser:**\n",
        "  - This parser allows the specification of an arbitrary Pydantic Model to query LLMs for outputs matching that schema. Pydantic's BaseModel functions similarly to a Python dataclass but includes type checking and coercion.\n",
        "\n",
        "- **JSON parser:**\n",
        "  - Users can specify an arbitrary JSON schema with this parser to ensure outputs from LLMs adhere to that schema. Pydantic can also be used to declare your data model here.\n",
        "\n",
        "- **CSV parser:**\n",
        "  - Useful for outputs requiring a list of items separated by commas. This parser facilitates the extraction of comma-separated values from model outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1-PDxOf1h87"
      },
      "source": [
        "### PydanticOutputParser\n",
        "\n",
        "This output parser allows users to specify an arbitrary Pydantic Model and query LLMs for outputs that conform to that schema.\n",
        "\n",
        "Keep in mind that large language models are non-deterministic! You'll have to use an LLM with sufficient capacity to generate well-formed responses.\n",
        "\n",
        "Use Pydantic to declare your data model. Pydantic's BaseModel is like a Python dataclass, but with actual type checking + coercion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fApMfO9zG4B"
      },
      "outputs": [],
      "source": [
        "# Updated import paths for prompt templates - using simplified paths:\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Define your desired data structure - like a python data class.\n",
        "class QueryResponse(BaseModel):\n",
        "    description: str = Field(description=\"A brief description of the topic asked by the user\")\n",
        "    pros: str = Field(description=\"3 bullet points showing the pros of the topic asked by the user\")\n",
        "    cons: str = Field(description=\"3 bullet points showing the cons of the topic asked by the user\")\n",
        "    conclusion: str = Field(description=\"One line conclusion of the topic asked by the user\")\n",
        "\n",
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = PydanticOutputParser(pydantic_object=QueryResponse)\n",
        "parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu_4ogkz0ABZ"
      },
      "outputs": [],
      "source": [
        "# langchain pre-generated output response formatting instructions\n",
        "print(parser.get_format_instructions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIrCkknp0FiO"
      },
      "outputs": [],
      "source": [
        "# create the final prompt with formatting instructions from the parser\n",
        "prompt_txt = \"\"\"\n",
        "             Answer the user query and generate the response based on the following formatting instructions\n",
        "\n",
        "             Format Instructions:\n",
        "             {format_instructions}\n",
        "\n",
        "             Query:\n",
        "             {query}\n",
        "            \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_txt,\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pCdfUpn0mdg"
      },
      "outputs": [],
      "source": [
        "# create a simple LCEL chain to take the prompt, pass it to the LLM, enforce response format using the parser\n",
        "chain = (prompt\n",
        "           |\n",
        "         chatgpt\n",
        "           |\n",
        "         parser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqYCylpw0u4L"
      },
      "outputs": [],
      "source": [
        "question = \"Tell me about Commercial Real Estate\"\n",
        "response = chain.invoke({\"query\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfLIwllb02ax"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As6-VKZE1W7z"
      },
      "outputs": [],
      "source": [
        "response.description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8rv_tK00_T-"
      },
      "outputs": [],
      "source": [
        "response.dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McK3BVa91CH8"
      },
      "outputs": [],
      "source": [
        "for k,v in response.dict().items():\n",
        "    print(f\"{k}:\\n{v}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbwJySEITJy"
      },
      "source": [
        "### JsonOutputParser\n",
        "\n",
        "This output parser allows users to specify an arbitrary JSON schema and query LLMs for outputs that conform to that schema.\n",
        "\n",
        "Keep in mind that large language models are non-deterministic! You'll have to use an LLM with sufficient capacity to generate well-formed responses.\n",
        "\n",
        "It is recommended use Pydantic to declare your data model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td0hVmQd3OVw"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "# Updated import paths for prompt templates - using simplified paths:\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser, JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "# Define your desired data structure - like a python data class.\n",
        "class QueryResponse(BaseModel):\n",
        "    description: str = Field(description=\"A brief description of the topic asked by the user\")\n",
        "    pros: str = Field(description=\"3 bullet points showing the pros of the topic asked by the user\")\n",
        "    cons: str = Field(description=\"3 bullet points showing the cons of the topic asked by the user\")\n",
        "    conclusion: str = Field(description=\"One line conclusion of the topic asked by the user\")\n",
        "\n",
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = JsonOutputParser(pydantic_object=QueryResponse)\n",
        "parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HJvSevb4XES"
      },
      "outputs": [],
      "source": [
        "# create the final prompt with formatting instructions from the parser\n",
        "prompt_txt = \"\"\"\n",
        "             Answer the user query and generate the response based on the following formatting instructions\n",
        "\n",
        "             Format Instructions:\n",
        "             {format_instructions}\n",
        "\n",
        "             Query:\n",
        "             {query}\n",
        "            \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_txt,\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObjYJrXh8Hj3"
      },
      "outputs": [],
      "source": [
        "# create a simple LCEL chain to take the prompt, pass it to the LLM, enforce response format using the parser\n",
        "chain = (prompt\n",
        "              |\n",
        "            chatgpt\n",
        "              |\n",
        "            parser)\n",
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3dlBUHh7682"
      },
      "outputs": [],
      "source": [
        "topic_queries = [\n",
        "    \"Tell me about commercial real estate\",\n",
        "    \"Tell me about Generative AI\"\n",
        "]\n",
        "\n",
        "topic_queries_formatted = [{\"query\": topic}\n",
        "                    for topic in topic_queries]\n",
        "topic_queries_formatted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnxspdUb8rSd"
      },
      "outputs": [],
      "source": [
        "responses = chain.map().invoke(topic_queries_formatted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLhLZnQi8xqq"
      },
      "outputs": [],
      "source": [
        "responses[0], type(responses[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJJMXDsV82Pd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(responses)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCPgsG1r3d_g"
      },
      "outputs": [],
      "source": [
        "for response in responses:\n",
        "  for k,v in response.items():\n",
        "    print(f\"{k}:\\n{v}\\n\")\n",
        "  print('-----')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6PZ6XE8IX8B"
      },
      "source": [
        "### CommaSeparatedListOutputParser\n",
        "\n",
        "This output parser can be used when you want to return a list of comma-separated items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsXC0ij886pg"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzt_1I48-JEt"
      },
      "outputs": [],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# And a query intented to prompt a language model to populate the data structure.\n",
        "prompt_txt = \"\"\"\n",
        "             Create a list of 5 different ways in which Generative AI can be used\n",
        "\n",
        "             Output format instructions:\n",
        "             {format_instructions}\n",
        "             \"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=prompt_txt)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1znuijS_Gds"
      },
      "outputs": [],
      "source": [
        "# create a simple LLM Chain - more on this later\n",
        "llm_chain = (prompt\n",
        "              |\n",
        "            chatgpt\n",
        "              |\n",
        "            output_parser)\n",
        "\n",
        "# run the chain\n",
        "response = llm_chain.invoke({'format_instructions': format_instructions})\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzm72WA_Jzxv"
      },
      "outputs": [],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "library versions updated:\n",
        "!pip install langchain==0.3.26\n",
        "!pip install langchain-openai==0.3.28\n",
        "!pip install langchain-community==0.3.27\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JXJJj7gUQmp3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}