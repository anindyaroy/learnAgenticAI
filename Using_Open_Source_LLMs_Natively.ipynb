{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anindyaroy/learnAgenticAI/blob/main/Using_Open_Source_LLMs_Natively.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Open Source LLMs Natively\n",
        "\n",
        "Here we will see briefly how you can use popular open source LLM APIs including\n",
        "\n",
        "- Hugging Face Transformers\n",
        "- Hugging Face Serverless Inference APIs\n",
        "- Hugging Face Inference Client\n",
        "- Groq Cloud"
      ],
      "metadata": {
        "id": "ZEUwOj-GYnm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "xWR3sUEoYj9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeG3DmGq4uV-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.53.2\n",
        "!pip install accelerate==1.9.0 # useful when using models with GPUs locally via huggingface\n",
        "!pip install groq==0.30.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Hugging Face Access Token\n",
        "\n",
        "Here you need to get an access token to be able to download or access models using Hugging Face's platform:\n",
        "\n",
        "- Hugging Face Access Token: Go [here](https://huggingface.co/settings/tokens) and create a key with write permissions. You need to setup an account which is totally free of cost.\n",
        "\n",
        "\n",
        "1. Go to [Settings -> Access Tokens](https://huggingface.co/settings/tokens) after creating your account and make sure to create a new access token with write permissions\n",
        "\n",
        "![](https://i.imgur.com/dtS6tFr.png)\n",
        "\n",
        "2. Remember to __Save__ your key somewhere safe as it will just be shown once as shown below. So copy and save it in a local secure file to use it later on. If you forget, just create a new key anytime.\n",
        "\n",
        "![](https://i.imgur.com/NmZmpmw.png)"
      ],
      "metadata": {
        "id": "eQCfDLCm9-nR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Hugging Face Access Token\n"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryheOZuXxa41"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "hf_key = getpass(\"Enter your Hugging Face Access Token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Key in Environment\n"
      ],
      "metadata": {
        "id": "mNOZE0ijAyHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = hf_key"
      ],
      "metadata": {
        "id": "bmZwlyyw8Bhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLMs Locally with Hugging Face\n",
        "\n",
        "This is if you want to download LLMs locally completely and run it without the need of sending your data to any external server. Do note you would need a GPU to run any of these models as even the smaller language models are still essentially quite big."
      ],
      "metadata": {
        "id": "d7CHjZfEAPlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain LLMs are gated like [Meta Llama 3.2 1B Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) so make sure to apply for access as shown below else you will get an error when using the model\n",
        "\n",
        "![](https://i.imgur.com/M88MOu5.png)"
      ],
      "metadata": {
        "id": "zWYLBpO06skJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the LLM locally using Huggingface"
      ],
      "metadata": {
        "id": "Bn_agb0scJIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "HJGTx5kaDiWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"Explain what is Generative AI in 2 bullet points\" },\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "3jf0oHjJ87Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to always refer to the [__documentation__](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate) where all the arguments of the generation pipeline are mentioned in detail. Most notably:\n",
        "\n",
        "- **max_length:** The maximum length of the sequence to be generated\n",
        "- **max_new_tokens:** The maximum numbers of tokens to generate, ignore the current number of tokens. Use either max_new_tokens or max_length but not both, they serve the same purpose\n",
        "- **do_sample:** Whether or not to use sampling. False means use greedy decoding i.e temperature=0\n",
        "- **temperature:** Between 0 - 1, The value used to module the next token probabilities. Higher temperature means the results may vary and be more creative"
      ],
      "metadata": {
        "id": "XJRNm1_bN5Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=1000)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "-Bu_Gkya9CQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipelines make it easier to send prompts\n",
        "\n",
        "You don't need to encode and decode your inputs and outputs everytime"
      ],
      "metadata": {
        "id": "k1-tFBvGcR0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_pipe = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"cuda\",\n",
        ")"
      ],
      "metadata": {
        "id": "8uOkv99AP2HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"Explain what is Generative AI in 2 bullet points\" },\n",
        "]"
      ],
      "metadata": {
        "id": "rWWiyC_T9tJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llama_pipe(chat, max_new_tokens=1000)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "u3SHOplN9kBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "SKqgAWfW90t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLMs via Hugging Face Inference APIs\n",
        "\n",
        "Thankfully HuggingFace has made its [__Inference API__](https://huggingface.co/docs/api-inference/quicktour) free to use with some basic rate limits etc. in place so you don't end up making unlimited requests on it's servers.\n",
        "\n",
        "The best part is you can access 150,000+ deep learning models without worrying about your infrastructure."
      ],
      "metadata": {
        "id": "x2OrAhrqAn4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Hugging Face Access Token\n"
      ],
      "metadata": {
        "id": "ivag8YTWBAHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBpPYcHeBAHw"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "hf_key = getpass(\"Enter your Hugging Face Access Token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Key in Environment\n"
      ],
      "metadata": {
        "id": "O2BFJxHFBAHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = hf_key"
      ],
      "metadata": {
        "id": "R0I1imf4BAHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create LLM API Access Function\n",
        "\n",
        "Here we create a basic function which can access any LLM API endpoint available on HuggingFace.\n",
        "\n",
        "For more details refer to the [detailed documentation](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) as needed."
      ],
      "metadata": {
        "id": "hgZzsDCZiN3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "headers = {\"Authorization\": \"Bearer \"+hf_key}\n",
        "\n",
        "def query(payload, API_URL):\n",
        "  response = requests.post(API_URL, headers=headers, json=payload)\n",
        "  print('API Response:', response)\n",
        "  return response.json()"
      ],
      "metadata": {
        "id": "-TRno7hBB_qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create LLM API Access Config\n",
        "\n",
        "Here we decide which LLMs we will access by getting their inference API endpoints.\n",
        "\n",
        "We also set some general configuration settings. You can find the [detailed documentation](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) here.\n",
        "\n",
        "Some useful config settings include:\n",
        "\n",
        "- max_new_tokens: The amount of new tokens to be generated in the response\n",
        "- do_sample: Whether or not to use sampling. False means use greedy decoding i.e temperature=0\n",
        "- temperature: Between 0 - 1, The value used to module the next token probabilities. Higher temperature means the results may vary and be more creative\n",
        "- return_full_text: If set to False, does not return your input prompt to the model\n",
        "- wait_for_model:  If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done\n",
        "- repetition_penalty: The more a token is used within generation the more it is penalized to not be picked in successive generation passes."
      ],
      "metadata": {
        "id": "9BPGezswkYNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_API_URL = \"https://router.huggingface.co/v1/chat/completions\" # updated to new endpoint as per HF changes https://huggingface.co/docs/huggingface_hub/v0.13.2/en/guides/inference\n",
        "# need to mention model provider as per new HF syntax https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct?inference_api=true&inference_provider=novita&language=python&client=requests\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct:novita\"\n",
        "params = {\n",
        "    \"wait_for_model\": True,\n",
        "    \"return_full_text\": False,\n",
        "    \"max_new_tokens\": 1000,\n",
        "}\n"
      ],
      "metadata": {
        "id": "WtpZr9DlXHPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =  \"Explain what is Generative AI in 2 bullet points\"\n",
        "# updated prompt input format as per https://huggingface.co/docs/huggingface_hub/v0.13.2/en/guides/inference\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "0aC_cWccBs3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updated payload format as per https://huggingface.co/docs/huggingface_hub/v0.13.2/en/guides/inference\n",
        "output = query(payload={\n",
        "                \"messages\": messages,\n",
        "                \"parameters\": params,\n",
        "                \"model\": model_name,\n",
        "                },\n",
        "                API_URL=HF_API_URL)\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "J4pQ0aCkBzYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLMs via Hugging Face Inference Client\n",
        "\n",
        "Thankfully HuggingFace has made its new [__Inference Client__](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client) free to use with some basic rate limits etc. in place so you don't end up making unlimited requests on its servers.\n",
        "\n",
        "The best part is you can access 150,000+ deep learning models without worrying about your infrastructure. Similar to the inference API"
      ],
      "metadata": {
        "id": "GDxl5-AHYwi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient"
      ],
      "metadata": {
        "id": "GZmgD-TzCzVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to refer to the [documentation](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient) at any time as needed for more details on function names, arguments and more."
      ],
      "metadata": {
        "id": "DzqLsnhNoqex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "client = InferenceClient(model=model_name, api_key=hf_key)\n",
        "\n",
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"Explain what is Generative AI in 2 bullet points\" },\n",
        "]\n",
        "\n",
        "response = client.chat_completion(chat, max_tokens=1000)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "CM-MNd4JCyOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "SPm2p0jRXQH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Grok API\n",
        "\n",
        "Here you need to get an access token to be able to access models using Grok's platform via APIs:\n",
        "\n",
        "- Groq API Key: Go [here](https://console.groq.com/keys) and create an API key. You need to setup an account which is totally free of cost. Also while Groq has a generous free tier, there are also paid plans if you are interested.\n",
        "\n",
        "\n",
        "1. Go to [Groq Cloud -> Create API Key](https://console.groq.com/keys) after creating your account and make sure to create a new API Key as shown\n",
        "\n",
        "![](https://i.imgur.com/tgHXlcV.png)\n",
        "\n",
        "2. Remember to __Save__ your key somewhere safe as it will just be shown once as shown below. So copy and save it in a local secure file to use it later on. If you forget, just create a new key anytime.\n",
        "\n",
        "![](https://i.imgur.com/Q27AgA1.png)"
      ],
      "metadata": {
        "id": "VGq7Jrnxo6Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Groq API Credentials\n"
      ],
      "metadata": {
        "id": "qvzfEaGRr6iJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tb0TO3Nr6iK"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "groq_key = getpass(\"Enter your Groq API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Open Source LLMs Directly via Groq API\n",
        "\n",
        "This is if you want to use it without wrappers like LangChain, we will show you how you use open LLMs like Meta Llama 3.2 Instruct using Groq APIs. The free tier should be good enough for most experiments."
      ],
      "metadata": {
        "id": "H7JCM1nrr6iL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Pricing\n",
        "\n",
        "Right now the best models to use include Mistral, Gemma 2 and Llama 3.1 and 3.2. Check out [pricing details here for free API](https://console.groq.com/settings/limits) and [here for paid API](https://groq.com/pricing/)\n",
        "\n",
        "![](https://i.imgur.com/JE8lfXV.png)"
      ],
      "metadata": {
        "id": "AxiB15nq9iJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Groq for Prompting Open Source LLMs"
      ],
      "metadata": {
        "id": "HabRE4PZAz80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "groq_client = Groq(api_key=groq_key)"
      ],
      "metadata": {
        "id": "nhJdZLOLvBzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_chatgroq(prompt, model=\"meta-llama/llama-guard-4-12b\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "ZWGDT9m2A2GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models keep getting updated and older ones get deprecated\n",
        "# refer to latest list here: https://console.groq.com/dashboard/limits\n",
        "prompt = 'Explain Generative AI in 2 bullet points'\n",
        "response = get_completion_chatgroq(prompt=prompt, model=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "yI_rv0EKBCGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Explain Generative AI in 2 bullet points'\n",
        "response = get_completion_chatgroq(prompt=prompt, model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ryaxUy_Q6UCA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}