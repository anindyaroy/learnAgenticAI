{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anindyaroy/learnAgenticAI/blob/main/M3_L12_IO_Advanced_Operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVraWpDTW8i0"
      },
      "source": [
        "# Advanced Operations for LLM Input / Output with LangChain\n",
        "\n",
        "This notebook covers the following operations:\n",
        "\n",
        "- LLM Cost Monitoring\n",
        "- Caching\n",
        "- Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install OpenAI, HuggingFace and LangChain dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSp3QoYtC-WV"
      },
      "outputs": [],
      "source": [
        "# Updated package versions for compatibility with latest APIs\n",
        "# !pip install langchain==0.3.21\n",
        "# !pip install langchain-openai==0.3.9\n",
        "# !pip install langchain-community==0.3.19\n",
        "\n",
        "\n",
        "!pip install langchain==0.3.26\n",
        "!pip install langchain-openai==0.3.28\n",
        "!pip install langchain-community==0.3.27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtBa7rlWJWH3"
      },
      "source": [
        "## Enter API Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6RD7As2sm8G"
      },
      "source": [
        "#### Enter your Open AI Key here\n",
        "\n",
        "You can get the key from [here](https://platform.openai.com/api-keys) after creating an account or signing in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ogxBkS6ZnnC"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Please enter your Open AI API Key here: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5rOqCyianbP"
      },
      "source": [
        "## Setup necessary system environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PIStD04Zp9p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4h0xywyJ3v7"
      },
      "source": [
        "## Chat Models and LLMs\n",
        "\n",
        "Large Language Models (LLMs) are a core component of LangChain. LangChain does not implement or build its own LLMs. It provides a standard API for interacting with almost every LLM out there.\n",
        "\n",
        "There are lots of LLM providers (OpenAI, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSdF6_R7J45Z"
      },
      "source": [
        "## Accessing Commercial LLMs like ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8nnrOGxZ2uZ"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "# Updated parameter name from model_name to model:\n",
        "chatgpt = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7smy7Cg49To"
      },
      "source": [
        "## Tracking LLM Costs\n",
        "\n",
        "Typically LLMs like ChatGPT charge you based on the number of tokens per request and response. You can track your token usage for specific calls. It is currently only implemented for the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTP3UdZz5MCk"
      },
      "outputs": [],
      "source": [
        "# Updated import path for callbacks:\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "prompt = \"\"\"Explain Generative AI in one line\"\"\"\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    response = chatgpt.invoke(prompt)\n",
        "    print(response.content)\n",
        "    print(cb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MYtDP8b5ctn"
      },
      "outputs": [],
      "source": [
        "cb.total_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnv4yvEC5gix"
      },
      "outputs": [],
      "source": [
        "cb.prompt_tokens, cb.completion_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYbmxO6j5j0W"
      },
      "outputs": [],
      "source": [
        "cb.total_cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIxhz_-rIc7A"
      },
      "source": [
        "## Caching in LangChain\n",
        "\n",
        "LangChain includes an optional caching layer for language model APIs (LLMs). This caching feature is beneficial for two main reasons:\n",
        "\n",
        "1. **Cost Efficiency:** By caching responses, you reduce the number of API calls made to LLM providers, especially helpful if you are frequently requesting the same completions. This can significantly lower costs.\n",
        "\n",
        "2. **Performance Improvement:** Caching can enhance your application's speed by decreasing the need for repeated API calls to the LLM provider, making interactions quicker and more efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTXKlly1I7r5"
      },
      "source": [
        "#### InMemoryCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25pMMHnJDds6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# integrations with other caching tools:\n",
        "# https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.cache\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.globals import set_llm_cache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"Explain to me what is mortgage\"\"\"\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "chatgpt.invoke(chat_template.format())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmBh0BeGFBOd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "chatgpt.invoke(chat_template.format())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVr9IJwI-w5"
      },
      "source": [
        "#### SQLite Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIhu_S5FFFm8"
      },
      "outputs": [],
      "source": [
        "# just to remove cache if it already exists, ignore if you get an error message below, that is normal (when cache doesnt exist)\n",
        "!rm langchain.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_ZoRnOQFJID"
      },
      "outputs": [],
      "source": [
        "# We can do the same thing with a SQLite cache\n",
        "from langchain.cache import SQLiteCache\n",
        "\n",
        "set_llm_cache(SQLiteCache(database_path=\"langchain.db\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfhl1XolFR81"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "prompt = \"\"\"Explain to me what is fractional real estate\"\"\"\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "chatgpt.invoke(chat_template.format())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSRJCYrxFZeU"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "chatgpt.invoke(chat_template.format())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esVD-ghbJCDD"
      },
      "source": [
        "## Streaming in LLMs\n",
        "\n",
        "All language model interfaces (LLMs) in LangChain implement the `Runnable` interface, which provides default methods such as `ainvoke`, `batch`, `abatch`, `stream`, and `astream`. This setup equips all LLMs with basic streaming capabilities.\n",
        "\n",
        "### Streaming Defaults:\n",
        "\n",
        "- **Synchronous Streaming:** By default, streaming operations return an `Iterator` that yields a single value, the final result from the LLM provider.\n",
        "- **Asynchronous Streaming:** Similarly, async streaming defaults to returning an `AsyncIterator` with the final result.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "- These default implementations do not support token-by-token streaming. For such detailed streaming, the LLM provider must offer native support. However, the default setup ensures that your code expecting an iterator of tokens will function correctly within these constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu7GykqNF7He",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Explain to me what is mortgage in detail with pros and cons\"\"\"\n",
        "chat_template = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "for chunk in chatgpt.stream(chat_template.format()):\n",
        "    print(chunk.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZgtSviWGAvv"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Explain to me what is mortgage in detail with pros and cons\"\"\"\n",
        "chat_template = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "response = []\n",
        "for chunk in chatgpt.stream(chat_template.format()):\n",
        "    print(chunk.content, end=\"\")\n",
        "    response.append(chunk.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmvaKjBg1PhO"
      },
      "outputs": [],
      "source": [
        "response[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABQd2OaN1Pjc"
      },
      "outputs": [],
      "source": [
        "print(''.join(response))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ayP_uvmER2G0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}