{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anindyaroy/learnAgenticAI/blob/main/Using_Open_Source_LLMs_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEUwOj-GYnm8"
      },
      "source": [
        "# Using Open Source LLMs with LangChain\n",
        "\n",
        "Here we will see briefly how you can use popular commercial LLM APIs with LangChain including\n",
        "\n",
        "- OpenAI GPT (Paid)\n",
        "- Google Gemini (Paid and Free)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWR3sUEoYj9R"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4JaMb3ZBlDF"
      },
      "outputs": [],
      "source": [
        "# Updated LangChain ecosystem packages:\n",
        "# - langchain==0.4.0 (from 0.3.4)\n",
        "# - langchain-huggingface==0.1.2 (from 0.1.0)\n",
        "# - langchain-groq==0.2.1 (from 0.2.0)\n",
        "# - transformers==4.65.1 (from 4.46.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YeG3DmGq4uV-",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.3.26                                                #0.4.0\n",
        "!pip install langchain-huggingface==0.3.1                                        # 0.1.2\n",
        "!pip install langchain-groq==0.3.6\n",
        "!pip install transformers==4.53.3                                                  # 4.65.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQCfDLCm9-nR"
      },
      "source": [
        "# Enter API Keys\n",
        "\n",
        "Here you need to get API keys from the following websites based on your LLM preference:\n",
        "\n",
        "- Hugging Face Access Token: Go [here](https://huggingface.co/settings/tokens) and create a key with write permissions. You need to setup an account which is totally free of cost.\n",
        "\n",
        "- Groq API Key: Go [here](https://console.groq.com/keys) and create an API key. You need to setup an account which is totally free of cost. Also while Groq has a generous free tier, there are also paid plans if you are interested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq63frTqnggW"
      },
      "source": [
        "## Load Hugging Face Access Token Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN3v667tnggW"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "hf_key = getpass(\"Enter your Hugging Face Access Token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNOZE0ijAyHB"
      },
      "source": [
        "## Configure Key in Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmZwlyyw8Bhm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_key\n",
        "os.environ[\"HF_TOKEN\"] = hf_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17FqNZFCKeOu"
      },
      "source": [
        "## Use LLMs locally with LangChain and Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OvhzE9TCU3a"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=model_id,\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=1000,\n",
        "        do_sample=False,\n",
        "        temperature=0,\n",
        "        return_full_text=False,\n",
        "    ),\n",
        "    device=0\n",
        ")\n",
        "llm.pipeline.tokenizer.pad_token = llm.pipeline.tokenizer.eos_token\n",
        "chat_llama = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBp5NrHK1r5o"
      },
      "outputs": [],
      "source": [
        "# Import path simplification in LangChain 0.4.0\n",
        "# Instead of: from langchain_core.prompts import ChatPromptTemplate\n",
        "# Now use the simplified path:\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "PROMPT = \"Explain {topic} in 2 bullets\"\n",
        "prompt = ChatPromptTemplate.from_template(PROMPT)\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "           |\n",
        "         chat_llama\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"AI\"})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvO6Evy54UyK"
      },
      "source": [
        "## Use LLMs with LangChain and Hugging Face Inference APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TA1i2rF4Zv_"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "llm_api = HuggingFaceEndpoint(\n",
        "    repo_id=model_id,\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=1000,\n",
        "    do_sample=False,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "chat_llama = ChatHuggingFace(llm=llm_api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM5Ahjal5FIA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "PROMPT = \"Explain {topic} in 2 bullets\"\n",
        "prompt = ChatPromptTemplate.from_template(PROMPT)\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "           |\n",
        "         chat_llama\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"AI\"})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvzfEaGRr6iJ"
      },
      "source": [
        "## Load Groq API Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tb0TO3Nr6iK"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "groq_key = getpass(\"Enter your Groq API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_DxI2-15i0p"
      },
      "source": [
        "## Configure Key in Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TS_-Qqd5i0p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7JCM1nrr6iL"
      },
      "source": [
        "## Use LLMs with LangChain and Groq API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBWMLH3V5agN"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "chat_llama = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0,\n",
        "    max_tokens=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k02fVCBY58Rn"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "PROMPT = \"Explain {topic} in 2 bullets\"\n",
        "prompt = ChatPromptTemplate.from_template(PROMPT)\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "           |\n",
        "         chat_llama\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"AI\"})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6tVLYjCxw9Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}